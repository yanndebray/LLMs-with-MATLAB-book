%[text] # Summarizing long text
%%
%[text] ### Setup
addpath("utils");
% Using venv on my Windows machine
pyenv(Version="..\env\Scripts\python.exe") %[output:72790d95]
%%
%[text] ### Youtube transcript
% DeepSeek, China, OpenAI, NVIDIA, xAI, TSMC, Stargate, and AI Megaclusters | Lex Fridman Podcast #459
video_id = "_1f-o0nqpEI";
% Python code input %[task:6918]
pycode = [...
"from youtube_transcript_api import YouTubeTranscriptApi",...
"transcript = YouTubeTranscriptApi.get_transcript(video_id)",...
"transcript = "" "".join(entry['text'].replace('\n', ' ') for entry in transcript)",...
"transcript"...
];

try %[output:group:829dcab7]
    [transcript] = pyrun(pycode, ... %[output:505a8046]
         [ "transcript" ], "video_id", video_id) %[output:505a8046]
catch ME6
    % Clear temporary variables from workspace and from Python
    clear pycode;
    if ME6.identifier == "MATLAB:Python:PyException"
        pyrun("del video_id");
    end
    rethrow(ME6)
end %[output:group:829dcab7]

% Clear temporary variables from workspace and from Python
clear pycode;
pyrun("del video_id"); %[task:6918]
tr = string(transcript) %[output:365242ab]
%%
%[text] Alternative using the py. syntax
% What Is Sentiment Analysis?
video_id = "hkwuaRdwEeI"
% Retrieve the transcript for the specified video ID using a Python function
tr = py.utils.youtube.get_transcript(video_id);
%%
% Save the transcript string into a text file
fileID = fopen('transcript2.txt', 'w');
fprintf(fileID, '%s\n', tr);
fclose(fileID);
%%
% Reload the transcript
tr = fileread('transcript2.txt');
tr = string(tr);
%%
%[text] ### Tokenization
%[text] Simple estimation of token count (1 token per 4 characters)
simpleTokenCounter(tr) %[output:5a30338a]
%%
%[text] Get more precision with the tokenizer from OpenAI
%[text] [How to count tokens with Tiktoken | OpenAI Cookbook](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
tiktoken = py.importlib.import_module('tiktoken');
encoding = tiktoken.encoding_for_model('gpt-4o-mini');
tok = encoding.encode(tr)
length(tok)
%%
%[text] Alternatively, use a utility function from my tokenization user-defined module:
py.utils.tokenization.num_tokens(tr)

%[appendix]{"version":"1.0"}
%---
%[metadata:view]
%   data: {"layout":"inline","rightPanelPercent":40}
%---
%[task:6918]
%   data: {"appState":"{\"PyInputModeCodeRadioBtn\":{\"Value\":true},\"PyInputModeFileRadioBtn\":{\"Value\":false},\"InputCode\":{\"Value\":[\"from youtube_transcript_api import YouTubeTranscriptApi\",\"transcript = YouTubeTranscriptApi.get_transcript(video_id)\",\"transcript = \\\" \\\".join(entry['text'].replace('\\\\n', ' ') for entry in transcript)\",\"transcript\"]},\"InputFile\":{\"Value\":\"\"},\"InputFileName\":\"\",\"OutputOptionsPanel\":{\"Collapsed\":true},\"OutputOptionsAllRadioBtn\":{\"Value\":true},\"OutputOptionsSelectRadioBtn\":{\"Value\":false},\"OutputOptionsGrid\":{\"RowHeight\":[30,0,30]},\"OutputOptionsSelectGrid\":{\"RowHeight\":[0,\"1x\",\"1x\",\"1x\"],\"Visible\":\"off\"},\"LargeOutputVarSizeDetected\":false,\"ForceSelectVars\":true,\"OutputOptionsOutputsListBox\":{\"Items\":[],\"Value\":[]},\"OutputOptionsSelectListBox\":{\"Items\":[\"transcript\"],\"Value\":[]},\"DisplayOutputCheckBox\":{\"Value\":true},\"codestr\":\"from youtube_transcript_api import YouTubeTranscriptApi\\ntranscript = YouTubeTranscriptApi.get_transcript(video_id)\\ntranscript = \\\" \\\".join(entry['text'].replace('\\\\n', ' ') for entry in transcript)\\ntranscript\",\"mlvars\":\"video_id\",\"pyvars\":\"transcript\"}","autorun":"1","collapsed":"0","outputs":"transcript","run":"section","taskClassDefFile":"python.internal.task.RunPythonCodeTask","uniqueId":"matlab\/RunPythonCode","variablesMap":"{\"ME\":\"ME6\",\"pycode\":\"pycode\",\"transcript\":\"transcript\"}","view":"controls-only"}
%---
%[output:72790d95]
%   data: {"dataType":"textualVariable","outputData":{"name":"ans","value":"  <a href=\"matlab:helpPopup('matlab.pyclient.PythonEnvironment')\" style=\"font-weight:bold\">PythonEnvironment<\/a> with properties:\n\n          Version: \"3.10\"\n       Executable: \"C:\\Users\\ydebray\\Downloads\\AI-agents-with-MATLAB-and-Python\\env\\Scripts\\python.exe\"\n          Library: \"C:\\Users\\ydebray\\AppData\\Local\\Programs\\Python\\Python310\\python310.dll\"\n             Home: \"C:\\Users\\ydebray\\Downloads\\AI-agents-with-MATLAB-and-Python\\env\"\n           Status: Loaded\n    ExecutionMode: OutOfProcess\n        ProcessID: \"12580\"\n      ProcessName: \"MATLABPyHost\"\n"}}
%---
%[output:505a8046]
%   data: {"dataType":"textualVariable","outputData":{"name":"transcript","truncation":{"vertical":true},"value":"  Python <a href=\"matlab:helpPopup('py.str')\" style=\"font-weight:bold\">str<\/a> with no properties.\n"}}
%---
%[output:365242ab]
%   data: {"dataType":"textualVariable","outputData":{"name":"tr","truncation":{"horizontal":true},"value":"\"- The following is a conversation with Dylan Patel and Nathan Lambert. Dylan runs SemiAnalysis, a well-respected research and analysis company that specializes in semiconductors, GPUs, CPUs, and AI hardware in general. Nathan is a research scientist at the Allen Institute for AI and is the author of the amazing blog on AI called Interconnects. They are both highly respected, read, and listened to by the experts, researchers, and engineers in the field of AI. And personally, I'm just a fan of the two of them. So, I used the DeepSeek moment that shook the AI world a bit as an opportunity to sit down with them and lay it all out. From DeepSeek, OpenAI, Google xAI, Meta, Anthropic, to Nvidia and DSMC, and to US, China, Taiwan relations, and everything else that is happening at the cutting edge of AI. This conversation is a deep dive into many critical aspects of the AI industry. While it does get super technical, we try to make sure that it's still accessible to folks outside of the AI field by defining terms, stating important concepts explicitly, spelling out acronyms, and in general, always moving across the several layers of abstraction and levels of detail. There is a lot of hype in the media about what AI is and isn't. The purpose of this podcast in part is to cut through the hype, through the bullshit, and the low resolution analysis, and to discuss in detail how stuff works and what the implications are. Let me also, if I may comment on the new OpenAI o3-mini reasoning model. The release of which we were anticipating during the conversation, and it did indeed come out right after. Its capabilities and costs are on par with our expectations as we stated. OpenAI o3-mini is indeed a great model, but it should be stated that DeeSeek-R1 has similar performance on benchmarks is still cheaper and it reveals its chain of thought reasoning, which o3-mini does not. It only shows a summary of the reasoning. Plus, R1 is open-weight and o3-mini is not. By the way, I got a chance to play with o3-mini. And anecdotal vibe check wise, I felt that o3-mini, specifically o3-mini-high is better than R1. Still for me personally, I find that Claude Sonnet 3.5 is the best model for programming, except for tricky cases, where I will use o1 Pro to brainstorm. Either way, many more better AI models will come, including reasoning models both from American and Chinese companies. They'll continue to shift the cost curve. But the quote, DeepSeek moment is indeed real. I think it will still be remembered five years from now as a pivotal event in tech history, due in part to the geopolitical implications, but for other reasons to, as we discuss in detail from many perspectives in this conversation. This is the \"Lex Fridman Podcast\". To support it, please check out our sponsors in the description. And now, dear friends, here's Dylan Patel and Nathan Lambert. A lot of people are curious to understand China's DeepSeek AI models, so let's lay it out. Nathan, can you describe what DeepSeek-V3 and DeepSeek-R1 are, how they work, how they're trained? Let's look at the big picture, and then we'll zoom in on the details. - Yeah, so DeepSeek-V3 is a new mixture of experts, transformer language model from DeepSeek who is based in China. They have some new specifics in the model that we'll get into. Largely this is a open-weight model and it's a instruction model like what you would use in ChatGPT. They also release what is called the base model, which is before these techniques of post-training. Most people use instruction models today and those are what's served in all sorts of applications. This was released on, I believe December 26th or that week. And then, weeks later, on January 20th, DeepSeek released DeepSeek-R1, which is a reasoning model, which really accelerated a lot of this discussion. This reasoning model, it has a lot of overlapping training steps to DeepSeek-V3, and it's confusing that you have a base model called V3 that you do something to, to get a chat model, and then you do some different things to get a reasoning model. I think a lot of the AI industry is going through this challenge of communications right now where OpenAI makes fun of their own naming schemes. They have GPT-4o, they have OpenAI o1. And there's a lot of types of models. So, we're gonna break down what each of them are. There's a lot of technical specifics on training, and go through 'em high level to specific, and go through each of them. - There's so many places we can go here, but maybe let's go to open-weights first. What does it mean for model to be open-weights and what are the different flavors of open source in general? - Yeah, so this discussion has been going on for a long time in AI. It became more important since ChatGPT or more focal since ChatGPT at the end of 2022. Open-weights is the accepted term for when model weights of a language model are available on the internet for people to download, those weights can have different licenses, which is the effectively the terms by which you can use the model. There are licenses that come from history and open source software. There are licenses that are designed by companies specifically, all of Llama, DeepSeek, Qwen, Mistral, these popular names in open-weight models, have some of their own licenses. It's complicated, 'cause not all the same models have the same terms. The big debate is on what makes a model open-weight. It's like why are we saying this term? It's a mouthful. It sounds close to open source but it's not the same. There's still a lot of debate on the definition and soul of open source AI. Open source software has a rich history on freedom to modify, freedom to take on your own, freedom for many restrictions on how you would use the software, and what that means for AI is still being defined. So, for what I do, I work at the Allen Institute for AI. We're a nonprofit. We want to make AI open for everybody and we try to lead on what we think is truly open source. There's not full agreement in the community, but for us, that means releasing the training data, releasing the training code, and then also having open-weights like this. And we'll get into the details of the models. And again and again, as we try to get deeper into how the models were trained, we will say things like the data processing, data filtering, data quality is the number one determinant of the model quality. And then, a lot of the training code is the determinant on how long it takes to train and how fast your experimentation is. So, without fully open source models where you have access to this data, it is hard to know or it's harder to replicate. So, we'll get into cost numbers for DeepSeek-V3 on mostly GPU hours and how much you could pay to rent those yourselves. But without the data, the replication cost is going to be far, far higher. And same goes for the code. - We should also say that this is probably one of the more open models out of the frontier models. - Yes. - So, in this full spectrum where probably the fullest open source, like you said, open code, open data, open-weights. This is not open code, this is probably not open data, and this is open-weights, and the licensing is MIT license, or it's... I mean, there's some nuance in the different models, but it's towards the free, in terms of the open source movement, these are the good guys. - Yeah. DeepSeek is doing fantastic work for disseminating understanding of AI. Their papers are extremely detailed in what they do. And for other teams around the world, they're very actionable in terms of improving your own training techniques. And we'll talk about licenses more. The DeepSeek-R1 model has a very permissive license. It's called the MIT license. That effectively means there's no downstream restrictions on commercial use, there's no use case restrictions. You can use the outputs from the models to create synthetic data. And this is all fantastic. I think the closest peer is something like Llama where you have the weights and you have a technical report. And the technical report is very good for Llama. One of the most read PDFs of the year last year is the Llama 3 paper, but in some ways, it's slightly less actionable. It has less details on the training specifics, I think less plots, and so on. And the Llama 3 license is more restrictive than MIT. And then, between the DeepSeek custom license and the Llama license, we could get into this whole rabbit hole. I think we'll make sure we want to go down the license rabbit hole before we do specifics. - Yeah, and so it should be stated that one of the implications that DeepSeek puts pressure on Llama and everybody else on OpenAI to push towards open source, and that's the other side of open source that you mentioned is how much is published in detail about it. So, how open are you with the insights behind the code? So, how good is the technical reports? Are they hand wavy or is there actual details in there? And that's one of the things that DeepSeek did well is they publish a lot of the details. - Yeah, especially in the DeepSeek-V3, which is their pre-training paper, they were very clear that they are doing interventions on the technical stack that go at many different levels. For example, to get highly efficient training, they're making modifications at or below the CUDA layer for Nvidia chips. I have never worked there myself. And there are a few people in the world that do that very well and some of them are at DeepSeek. And these types of people are at DeepSeek and leading American Frontier Labs, but there are not many places. - To help people understand the other implication of open-weights, just there's a topic we return to often here. So, there's a fear that China, the nation, might have interest in stealing American data, violating privacy of American citizens. What can we say about open-weights to help us understand what the weights are able to do - Yeah. - in terms of stealing people's data? - Yeah, so these weights that you can download from Hugging Face or other platforms are very big matrices of numbers. You can download them to a computer in your own house that has no internet and you can run this model, and you're totally in control of your data. That is something that is different than how a lot of language model usage is actually done today, which is mostly through APIs where you send your prompt to GPUs run by certain companies, and these companies will have different distributions and policies on how your data is stored, if it is used to train future models, where it is stored, if it is encrypted, and so on. So, the open-weights are you have your fate of data in your own hands and that is something that is deeply connected to the soul of open source. - So, it's not the model that steals your data, it's whoever's hosting the model, which could be China if you're using the DeepSeek app, or it could be Perplexity. You're trusting them with your data. Or OpenAI, you're trusting them with your data. And some of these are American companies, some of of these are Chinese companies, but the model itself is not doing the stealing, it's the host. All right. So, back to the basics. What's the difference between DeepSeek-V3 and DeepSeek-R1? Can we try to lay out the confusion potential? - Yes. So, for one, I have very understanding of many people being confused by these two model names. So, I would say the best way to think about this is that when training a language model, you have what is called pre-training, which is when you're predicting the large amounts of mostly internet text. You're trying to predict the next-token. And what to know about these new DeepSeek models is that they do this internet large-scale pre-training once to get what is called DeepSeek-V3 base. This is a base model. It's just going to finish your sentences for you. It's going to be harder to work with than ChatGPT. And then, what DeepSeek did is they've done two different post-training regimes to make the models have specific desirable behaviors. So, what is the more normal model in terms of the last few years of AI and instruct model, a chat model, a, quote, unquote, \"aligned model\", a helpful model, there are many ways to describe this, is more standard post-training. So, this is things like instruction tuning, reinforcement learning from human feedback. We'll get into some of these words. And this is what they did to create the DeepSeek-V3 model. This was the first model to be released and it is very highly performant, competitive with GPT-4, Llama 405b, so on. And then, when this release was happening, we don't know their exact timeline, or soon after they were finishing the training of a different training process from the same next-token prediction-based model that I talked about, which is when this new reasoning training that people have heard about comes in, in order to create the model that is called DeepSeek-R1. The R through this conversation is good for grounding for reasoning. And the name is also similar to OpenAI's o1, which is the other reasoning model that people have heard about. And we'll have to break down the training for R1 in more detail, because for one, we have a paper detailing it, but also, it is a far newer set of techniques for the AI community. So, it is a much more rapidly evolving area of research. - Maybe we should also say the big two categories of training of pre-training and post-training. These umbrella terms that people use. So, what is pre-training and what is post-training, and what are the different flavors of things underneath post-training umbrella? - Yeah, so pre-training, I'm using some of the same words to really get the message across, is you're doing what is called autoregressive prediction to predict the next-token in a series of documents. This is done over standard practice is trillions of tokens. So, this is a ton of data that is mostly scraped from the web. And some of DeepSeek's earlier papers, they talk about their training data being distilled for math, I shouldn't use this word yet, but taken from Common Crawl, and that's a public access that anyone listening to this could go download data from the Common Crawl website. This is a crawler that is maintained publicly. Yes, other tech companies eventually shift to their own crawler and DeepSeek likely has done this as well, as most Frontier Labs do. But this sort of data is something that people can get started with and you're just predicting text in a series of documents. This can be scaled to be very efficient and there's a lot of numbers that are thrown around in AI training, like how many floating point operations or FLOPS are used. And then, you can also look at how many hours of these GPUs that are used. And it's largely one loss function taken to a very large amount (chuckles) of compute usage. You set up really efficient systems. And then, at the end of that, you have this base model. And pre-training is where there is a lot more of complexity in terms of how the process is emerging or evolving, and the different types of training losses that you'll use. I think this is a lot of techniques grounded in the natural language processing literature. The oldest technique which is still used today is something called instruction tuning, or also known as supervised fine-tuning. These acronyms will be IFT or SFT that people really go back and forth throughout them, and I'll probably do the same, which is where you add this formatting to the model, where it knows to take a question that is like explain the history of the Roman Empire to me. Or something, a sort of question you'll see on Reddit or Stack Overflow, and then the model will respond in a information-dense but presentable manner. The core of that formatting is in this instruction tuning phase. And then, there's two other categories of loss functions that are being used today. One I'll classify as preference fine-tuning. Preference fine-tuning is a generalized term for what came out of reinforcement learning from human feedback, which is RLHF. This reinforcement learning from human feedback is credited as the technique that helped ChatGPT breakthrough. It is a technique to make the responses that are nicely formatted, like these Reddit answers, more in tune with what a human would like to read. This is done by collecting pairwise preferences from actual humans out in the world to start. And now, AIs are also labeling this data and we'll get into those trade-offs. And you have this kind of contrastive loss function between a good answer and a bad answer. And the model learns to pick up these trends. There's different implementation ways. You have things called reward models. You could have direct alignment algorithms. There's a lot of really specific things you can do. But all of this is about fine-tuning to human preferences. And the final stage is much newer and will link to what is done in R1. And these reasoning models is I think OpenAI's name for this. They had this new API in the fall, which they called the reinforcement fine-tuning API. This is the idea that you use the techniques of reinforcement learning, which is a whole framework of AI. There's a deep literature here. To summarize, it's often known as trial and error learning or the subfield of AI where you're trying to make sequential decisions in a certain potentially noisy environment. There's a lot of ways we could go down that. But fine-tuning language models where they can generate an answer. And then, you check to see if the answer matches the true solution. For math or code, you have an exactly correct answer for math. You can have unit tests for code. And what we're doing is we are checking the language model's work and we're giving it multiple opportunities on the same questions to see if it is right. And if you keep doing this, the models can learn to improve in verifiable domains to a great extent. It works really well. It's a newer technique in the academic literature. It's been used at Frontier Labs in the US that don't share every detail for multiple years. So, this is the idea of using reinforcement learning with language models, and it has been taking off, especially in this DeepSeek moment. - And we should say that there's a lot of exciting stuff going on the, again, across the stack. But the post-training probably this year, there's going to be a lot of interesting developments in the post-training. We'll talk about it. I almost forgot to talk about the difference between DeepSeek-V3 and R1 on the user experience side. So, forget the technical stuff, forget all of that. Just people that don't know anything about AI, they show up like what's the actual experience, what's the use case for each one when they actually type and talk to it? - Yeah. - What is each good at and that kind of thing. - So, let's start with DeepSeek-V3. Again, it's more people would have tried something like it. You ask it a question, it'll start generating tokens very fast, and those tokens will look like a very human legible answer. It'll be some sort of markdown list. It might have formatting to help you draw to the core details in the answer. And it'll generate tens to hundreds of tokens. Say token is normally a word for common words or a sub-word part in a longer word. And it'll look like a very high quality Reddit or Stack Overflow answer. These models are really getting good at doing these across a wide variety of domains, I think. Even things that if you're an expert, things that are close to the fringe of knowledge, they will still be fairly good at, I think. Cutting edge AI topics that I do research on, these models are capable for study aide, and they're regularly updated. Where this changes is with the DeepSeek-R1, what is called these reasoning models, is when you see tokens coming from these models to start, it will be a large chain of thought process. We'll get back to chain of thought in a second, which looks like a lot of tokens, where the model is explaining the problem. The model will often break down the problem and be like, okay, they asked me for this, let's break down the problem. I'm going to need to do this. And you'll see all of this generating from the model. It'll come very fast in most user experiences. These APIs are very fast. So, you'll see a lot of tokens, a lot of words show up really fast. It'll keep flowing on the screen and this is all the reasoning process. And then, eventually, the model will change its tone in R1 and it'll write the answer, where it summarizes its reasoning process and writes a similar answer to the first types of model. But in DeepSeek's case, which is part of why this was so popular even outside the AI community, is that you can see how the language model is breaking down problems. And then, you get this answer. On a technical side, they train the model to do this specifically, where they have a section which is reasoning, and then it generates a special token, which is probably hidden from the user most of the time, which says, okay, I'm starting the answer. So, the model is trained to do this two-stage process on its own. If you use a similar model in say, OpenAI, OpenAI's user interface is trying to summarize this process for you nicely by showing the sections that the model is doing, and it'll click through, it'll say breaking down the problem, making X calculation, cleaning the result, and then the answer will come for something like OpenAI. - Maybe it's useful here to go through like an example of it, DeepSeek-R1 reasoning. - Yeah. So, if you're looking at the screen here, what you'll see is a screenshot of the DeepSeek chat app. And at the top is Thought for 157 seconds with the dropdown arrow. Underneath that, if we were in an app that we were running, the dropdown arrow would have the reasoning. - So, in this case, the question, the specific question which, I'm philosophically\/pothead-inclined, so this is asking DeepSeek-R1 for one truly novel insight about humans. And it reveals the reasoning. And basically, the truly novel aspect is what's pushing the reasoning to constantly, the model asking itself, is this truly novel? So, it's actually challenging itself to be more novel, more counterintuitive, less cringe, I suppose. So, some of the reasoning says, this is just snapshots. Alternatively, humans have a unique meta-emotion where they feel emotions about their own emotions, e.g, feeling guilty about being angry. This recursive emotional layering creates complex motivational drives that don't exist in other animals. The inside is that human emotions are nested. So, it's like, it's reasoning through how humans feel emotions, it's reasoning about meta-emotions. - It's gonna have pages and pages of this. - Yeah. - It's almost too much to actually read, but it's nice to skim as it's coming. - It's a stream of con... It's a James Joyce-like stream of consciousness. And then, it goes, wait, the user wants something that's not seen anywhere else. Let me dig deeper. And consider the human ability to hold contradictory beliefs simultaneously. Cognitive dissonance is known, but perhaps the function is to allow flexible adaptation, so on and so forth. That really captures the public imagination, that holy shit, this isn't... I mean, intelligence\/almost like an inkling of sentience, because you're thinking through, you're self-reflecting, you're deliberating. And the final result of that after 157 seconds is humans instinctively convert selfish desires into cooperative systems by collectively pretending abstract rules, money, laws, rights, are real. These shared hallucinations act as, quote, games, where competition is secretly redirected to benefit the group, turning conflict into society's fuel. Pretty profound. I mean, you know? Comment- - This is a potential digression, but a lot of people have found that these reasoning models can sometimes produce much more eloquent text. That is at least interesting example, I think, depending on how open-minded you are, you find language models interesting or not, and there's a spectrum there. - Well, it's some of the... We'll talk about different benchmarks, and so on. But some is just a vibe. Like that in itself is a, let's say, quote, fire tweet. - Yeah. (laughs) - If I'm trying to produce something, where people are like, oh shit, okay, so that's chain of thought. We'll probably return to it more. How were they able to achieve such low cost on the training and the inference? Maybe you could talk the training first. - Yeah, so there's two main techniques that they implemented that are probably the majority of their efficiency. And then, there's a lot of implementation details that maybe we'll gloss over or get into later that contribute to it. But those two main things are, one, is they went to a mixture of experts model, which we'll define in a second. And then, the other thing is that they invented this new technique called MLA latent attention. Both of these are big deals. Mixture of experts is something that's been in the literature for a handful of years. And OpenAI with GPT-4 was the first one to productize a mixture of experts model. And what this means is when you look at the common models around that most people have been able to interact with that are open. Think Llama, Llama is a dense model. i.e, every single parameter or neuron is activated as you're going through the model for every single token you generate. Now, with a mixture of experts model, you don't do that. How does the human actually work is like, oh, well, my visual cortex is active when I'm thinking about vision tasks and other things. My amygdala is when I'm scared. These different aspects of your brain are focused on different things. A mixture of experts models attempts to approximate this to some extent. So, nowhere close to what a brain architecture is, but different portions of the model activate. You'll have a set number of experts in the model and a set number that are activated each time. And this dramatically reduces both your training and inference cost. Because now, if you think about the parameter count as the total embedding space for all of this knowledge that you're compressing down during training, one, you're embedding this data in, instead of having to activate every single parameter, every single time you're training or running inference. Now, you can just activate on a subset. And the model will learn which expert to route to for different tasks. And so, this is a humongous innovation in terms of, hey, I can continue to grow the total embedding space of parameters. And so, DeepSeek's model is 600 something billion parameters. Relative to Llama 405b, it's 405 billion parameters. Llama relative to Llama 70b, it's 70 billion parameters. So, this model technically has more embedding space for information, to compress all of the world's knowledge that's on the internet down. But at the same time, it is only activating around 37 billion of the parameters. So, only 37 billion of these parameters actually need to be computed every single time you're training data or inferencing data out of it. And so, versus again a Llama model, 70 billion parameters must be activated or 405 billion parameters must be activated. So, you've dramatically reduced your compute cost when you're doing training and inference with this mixture of experts architecture. - So, we break down where it actually applies and go into the transformer. Is that useful? - Let's go, let's go into the transformer. - Okay. So, the transformer (Lex laughing) is a thing that is talked about a lot and we will not cover every detail. Essentially, the transformer is built on repeated blocks of this attention mechanism, and then a traditional dense, fully connected multi-layer perception, whatever word you want to use for your normal neural network. And you alternate these blocks. There's other details. And where mixture of experts is applied is that this dense model. The dense model holds most of the weights if you count them in a transformer model. So, you can get really big gains from those mixture of experts on parameter efficiency at training and inference, because you get this efficiency by not activating all of these parameters. - [Lex] We should also say that a transformer is a giant neural network. - Yeah. - And then, there's for 15 years now, there's what's called the deep learning revolution. Networks gotten larger and larger. And a certain point, the scaling laws appeared where people realized... - This is a scaling law shirt, by the way. (group laughing) - Representing. Scaling laws where it became more and more formalized that bigger is better across multiple dimensions of what bigger means. But these are all neural networks we're talking about. And we're talking about different architectures of how to construct these neural networks such that the training and the inference on them is super efficient. - Yeah, every different type of model has a different scaling law for it, which is effectively for how much compute you put in, the architecture will get to different levels of performance at test tasks. A mixture of experts is one of the ones at training time. Even if you don't consider the inference benefits, which are also big. At training time, your efficiency with your GPUs is dramatically improved by using this architecture if it is well-implemented. So, you can get effectively the same performance model and evaluation scores with numbers like 30% less compute. I think there's gonna be a wide variation, depending on your implementation details and stuff. But it is just important to realize that this type of technical innovation is something that gives huge gains. And I expect most companies that are serving their models to move to this mixture of experts implementation. Historically, the reason why not everyone might do it is because it's a implementation complexity, especially when doing these big models. So, this is one of the things that DeepSeek gets credit for is they do this extremely well. They do a mixture of experts extremely well. This architecture for what is called DeepSeekMoE. MoE is the shortened version of mixture of experts, is multiple papers old. This part of their training infrastructure is not new to these models alone, and same goes for what Dylan mentioned, with multi-head latent attention. This is all about reducing memory usage during inference. And same things during training, by using some fancy low rank approximation math. If you get into the details with this latent attention, it's one of those things I look at, and it's like, okay, they're doing really complex implementations, 'cause there's other parts of language models such as embeddings that are used to extend the context length. The common one that DeepSeek used is rotary positional impendings, which is called RoPE. And if you want to use RoPE with a normal MoE, it's a sequential thing. You take two of the attention matrices and you rotate them by a complex value rotation, which is a matrix multiplication. With DeepSeek's MLA, with this new attention architecture, they need to do some clever things, because they're not set up the same and it just makes the implementation complexity much higher. So, they're managing all of these things, and these are probably the sort of things that OpenAI, these closed labs are doing. We don't know if they're doing the exact same techniques, but they actually shared them with the world, which is really nice to be like, this is the cutting edge of efficient language model training. - And some of this requires low level engineering, just is a giant mess in trickery. So, as I understand, that went below CUDA, so they go super low programming of GPUs. - Effectively, Nvidia builds this library called NCCL. In which, when you're training a model, you have all these communications between every single layer of the model and you may have over 100 layers. - What does the NCCL stand for? It's N-C-C-L? - Nvidia Communications Collectives Library. - [Lex] Nice. (Nathan laughing) Damn. - And so, (group laughing) when you're training a model, you're gonna have all reduces and all gathers. Between each layer, between the multi-layer perception or feedforward network and the attention mechanism you'll have, you'll have basically the model synchronized. Or you'll have all reduce and all gather. And this is a communication between all the GPUs in the network, whether it's in training or inference. So, Nvidia has a standard library. This is one of the reasons why it's really difficult to use anyone else's hardware for training is because no one's really built a standard communications library. And Nvidia's done this at a higher level. At DeepSeek, because they have certain limitations around the GPUs that they have access to, the interconnects are limited to some extent by the restrictions of the GPUs that were shipped into China legally, not the ones that are smuggled, but legally shipped in, that they used to train this model. They had to figure out how to get efficiencies. And one of those things is that instead of just calling the Nvidia library NCCL, they instead created their, they scheduled their own communications, which some of the labs do. Meta talked about in Llama 3 how they made their own custom version of NCCL. They didn't talk about the implementation details. This is some of what they did. Probably not as well as, maybe not as well as DeepSeek because DeepSeek necessity is the mother of innovation and they had to do this. Whereas in the ca... OpenAI has people that do this sort of stuff, Anthropic, et cetera. But DeepSeek certainly did it publicly and they may have done it even better because they were gimp on a certain aspect of the chips that they have access to. And so, they scheduled communications by scheduling specific SMs. SMs you could think of as like the core on a GPU. So, there's hundreds of cores or there's a bit over 100 cores, SMs, on a GPU, and they were specifically scheduling, hey, which ones are running the model? Which ones are doing all reduce? Which one are doing all gather? And they would flip back and forth between them. And this requires extremely low level programming. - This is what NCCL does automatically or other Nvidia libraries handle this automatically usually. - Yeah, exactly. And so, technically, they're using PTX, which is like, you could think of it as like an assembly type language. It's not exactly that or instruction set. Coding directly to assembly or instruction set. It's not exactly that, but that's still part of technically CUDA. But it's like, do I wanna write in Python, PyTorch equivalent, and call Nvidia libraries? Do I want to go down to the C level. Or in code, even lower level? Or do I wanna go all the way down to the assembly or ISA level? And there are cases where you go all the way down there at the very big labs, but most companies just do not do that because it's a waste of time and the efficiency gains you get are not worth it. But DeepSeek's implementation is so complex. Especially with their mixture of experts. People have done mixture of experts, but they're generally 8, 16 experts. And they activate too. So, one of the words that we like to use is sparsity factor or usage. So, you might have four, one fourth of your model activate. And that's what misdraws mixed role model. They're model that really catapulted them to like, oh my god, they're really, really good. OpenAI has also had models that are MoE, and so have all the other labs that are major closed. But what DeepSeek did that maybe only the leading labs have only just started recently doing is have such a high sparsity factor. It's not one fourth of the model. Two out of eight experts activating every time you go through the model. It's 8 out of 256. - And there's different implementations from mixture of experts where you can have some of these experts that are always activated, which this just looks like a small neural network. And then, all the tokens go through that. And then, they also go through some that are selected by this routing mechanism. And one of the innovations in DeepSeek's architecture is that they change the routing mechanism in mixture of expert models. There's something called an auxiliary loss, which effectively means during training, you want to make sure that all of these experts are used across the tasks that the model sees. Why there can be failures in mixture of experts is that when you're doing this training, the one objective is token prediction accuracy. And if you just let turning go with a mixture of expert model on your own, it can be that the model learns to only use a subset of the experts. And in the MoE literature, there's something called the auxiliary loss, which helps balance them. But if you think about the loss functions of deep learning, this even connects to the bitter lesson, is that you want to have the minimum inductive bias in your model to let the model learn maximally. And this auxiliary loss, this balancing across experts could be seen as intention with the prediction accuracy of the tokens. So, we don't know the exact extent that the DeepSeek MoE change, which is instead of doing an auxiliary loss, they have an extra parameter in their routing, which after the batches, they update this parameter to make sure that the next batches all have a similar use of experts. And this type of change can be big, it can be small, but they add up over time. And this is the sort of thing that just points to them innovating. And I'm sure all the labs that are training big MoEs are looking at this sort of things, which is getting away from the auxiliary loss. Some of them might already use it, but you keep accumulating gains. And we'll talk about the philosophy of training and how you organize these organizations. And a lot of it is just compounding small improvements over time in your data, in your architecture, in your post-training, and how they integrate with each other. And DeepSeek does the same thing and some of 'em are shared or a lot, we have to take them on face value that they share their most important details. The architecture and the weights are out there, so we're seeing what they're doing, and it adds up. - Going back to the efficiency and complexity point. It's 32 versus 4 for mixed draw and other MoE models that have been publicly released. So, this ratio is extremely high. And what Nathan was getting at there was, when you have such a different level of sparsity, you can't just have every GPU have the entire model. The model's too big, there's too much complexity there. So, you have to split up the model with different types of parallelism. And so, you might have different experts on different GPU nodes, but now what happens when this set of data that you get, hey, all of it looks like this one way and all of it should route to one part of my model. So, when all of it routes to one part of the model, then you can have this overloading of a certain set of the GPU resources or a certain set of the GPUs, and then the rest of the training network sits idle, because all of the tokens are just routing to that. So, this is the biggest complexity, one of the big complexities with running a very sparse mixture of experts model, i.e, this 32 ratio versus this 4 ratio, is that you end up with so many of the experts just sitting there idle. So, how do I load balance between them? How do I schedule the communications between them? This is a lot of the extremely low level detailed work that they figured out in the public first and potentially second or third in the world, and maybe even first in some cases. - What lesson do you, in the direction of the bitter lesson, do you take from all of this? Is this going to be the direction where a lot of the gain is going to be, which is this kind of low level optimization? Or is this a short-term thing where the biggest gains will be more on the algorithmic high level side of post-training? Is this like a short-term leap because they've figured out like a hack, because constraints, necessity is the mother of invention, or is there still a lot of gains? - I think we should summarize what the bitter lesson actually is about, is that the bitter lesson, - Okay. - essentially, if you paraphrase it, is that the types of training that will win out in deep learning as we go are those methods that are which are scalable in learning and search is what it calls out. And the scale word gets a lot of attention in this. The interpretation that I use is effectively to avoid adding the human priors to your learning process. And if you read the original essay, this is what it talks about is how researchers will try to come up with what clever solutions to their specific problem that might get them small gains in the short-term while simply enabling these deep learning systems to work efficiently. And for these bigger problems in the long-term might be more likely to scale and continue to drive success. And therefore, we were talking about relatively small implementation changes to the mixture of experts model. And therefore, it's like, okay, we will need a few more years to know if one of these were actually really crucial to the bitter lesson. But the bitter lesson is really this long-term arc of how simplicity can often win. And there's a lot of sayings in the industry, like the models just wanna learn. You have to give them the simple loss landscape where you put compute through the model, and they will learn and getting barriers out of the way. - That's where the power, something like NCCL comes in, where standardized code that could be used by a lot of people to create sort of simple innovations that can scale. Which is why the hacks, I imagine the code base for DeepSeek is probably a giant mess. - I'm sure they have, DeepSeek definitely has code bases that are extremely messy, where they're testing these new ideas, multi-head latent attention, probably could start in something like a Jupyter Notebook or somebody try something on a few GPUs. And that is really messy. But the stuff that trains the DeepSeek-V3 and DeepSeek-R1, those libraries, if you were to present them to us, I would guess are extremely high quality code. - It's a high quality readable code. Yeah. - I think there is one aspect to note though. Is that there is the general ability for that to transfer across different types of runs. You may make really, really high quality code for one specific model architecture at one size. - Yeah. - And then, that is not transferable to, hey, when I make this architecture tweak, everything's broken again. That's something that could be with their specific low level coding of scheduling SMs, is specific to this model architecture and size. And whereas like Nvidia's collectives library is more like, hey, it'll work for anything. You wanna do an all reduce, great, I don't care what your model architecture is, it'll work. And you're giving up a lot of performance when you do that in many cases. But it's worthwhile for them to do the specific optimization for the specific run, given the constraints that they have regarding compute. - I wonder how stressful it is to like, these frontier models, like initiate training, like to have the code- - [Nathan] Push the button. - to push the button (Nathan laughing) that you're now spending a large amount of money and time to train this. There must be a lot of innovation on the debugging stage of making sure there's no issues that you're monitoring and visualizing every aspect of the training, all that kind of stuff. - When people are training, they have all these various dashboards, but the most simple one - Yeah. - is your loss. - Right. - And it continues to go down. But in reality, especially with more complicated stuff like MoE, the biggest problem with it, or FP8 training, which is another innovation, going to a lower precision number format, i.e, less accurate, is that you end up with loss spikes. And no one knows why the loss spike happened. And for a lot- - Some of them you do. - Some of them you do. - Some of them are bad data. I give AI2's example of what blew up our earlier models is a subreddit called Microwave Gang. We love to shoutout this out. (Lex laughing) It's a real thing. You can pull up Microwave Gang. Essentially, it's a subreddit where everybody makes posts that are just the letter M. So, it's like, mm. So, there's extremely long sequences of the letter M, and then the comments are like, beep beep. 'Cause it's in the microwave end. - Yeah. - If you pass this into a model that's trained to be a normal producing text, it's extremely high loss. 'Cause normally, - Yeah. - you see an M. (Lex laughing) You don't predict Ms for a long time. So, this is something that cause a loss spikes for us. But when you have much like, this is old, this is not recent. And when you have more mature data systems, that's not the thing that causes the loss spike. And what Dylan is saying is true, but it's levels to this sort of idea. - With regards to the stress, (Nathan and Lex laughing) these people are like, you'll go out to dinner with a friend that works at one of these labs. And they'll just be like (Nathan laughing) looking at their phone every 10 minutes, and they're not like... It's one thing if they're texting, but they're just like, - Yeah. - is the loss- - It's literal. The tokens per second loss, not blown up. They're just watching this. (chuckles) - And the heart rate goes up if there's a spike. - And some level of spikes is normal. It'll recover and be back. Sometimes a lot of the old strategy was like, you just stop the run, restart from an old version, and then change the data mix, and then it keeps going. - There are even different types of spikes. So, Dirk Groeneveld has a theory of AI too that's like fast spikes and slow spikes, where there are sometimes where you're looking at the loss and there are other parameters, you can see it start to creep up, and then blow up. And that's really hard to recover from. So, you have to go back much further. So, you have the stressful period, where it's like flat or it might start going up and you're like, what do I do? Whereas there are also lost spikes that are, it looks good. And then, there's one spiky data point. And what you could do is you just skip those. You see that there's a spike. You're like, okay, I can ignore this data, don't update the model, and do the next one, and it'll recover quickly. But these like untrickier implementation, so as you get more complex in your architecture and you scale up to more GPUs, you have more potential for your loss blowing up. So, it's like there's a distribution. - And then, the whole idea of grokking also comes in. It's like, just because it's slowed down from improving and loss doesn't mean it's not learning, because all of a sudden, it could be like this and it could just spike down in loss again because it learned, truly learned something. And it took some time for it to learn that. It's not like a gradual process. And that's what humans are like, that's what models are like. It's really a stressful task as you mentioned. - And the whole time, the dollar count is going up. - Every company has failed runs. You need failed run to push the envelope on your infrastructure. So, a lot of news cycles are made of X company had Y failed run. Every company that's trying to push the frontier of AI has these. So, yes, it's noteworthy because it's a lot of money and it can be week to month setback, but it is part of the process. - But how do you get, if you're DeepSeek, how do you get to a place where holy shit, there's a successful combination of hyperparameters? - A lot of small failed runs. - And so, rapid iteration (Nathan chuckles) through failed runs, until- - Yeah, and successful ones. You just- - And then, you build up some intuition like this, this mixture of expert works, and then this implementation of MLA works. - Key hyperparameters like learning rate and regularization and things like this. And you find the regime that works for your code base. Talking to people at Frontier Labs, there's a story that you can tell where training language models is kind of a path that you need to follow. So, you need to unlock the ability to train a certain type of model or a certain scale. And then, your code base and your internal knowhow of which hyperparameters work for it is known. And you look at the DeepSeek papers and models, they've scaled up, they've added complexity, and it's just continuing to build the capabilities that they have. - There's the concept of a YOLO run. (Nathan laughing) So, YOLO, you only live once. - Yep. - And what it is, is like, there's all this experimentation you do at the small-scale. Research ablations, you have your Jupyter Notebook with, you're experimenting with MLA on three GPUs or whatever, and you're doing all these different things like, hey, do I do 4 expert, 4 active experts, 128 experts, do I arrange the experts this way? All these different model architecture things you're testing at a very small-scale. Couple researchers, few GPUs, tens of GPUs, hundreds of GPUs, whatever it is. And then, all of a sudden, you're like, okay guys, no more fucking around. No more screwing around. Everyone, take all the resources we have, let's pick what we think will work, and just go for it. YOLO. - Yeah. - And this is where that sort of stress comes in is like, well, I know it works here, but some things that work here don't work here, and some things that work here don't work down here - Yeah. - in this terms of scale. So, it's really truly a YOLO run, and there's this discussion of certain researchers just have this methodical nature. They can find the whole search space and figure out all the ablations of different research, and really see what is best. And there's certain researchers who just like, have that innate gut instinct of like, this is the YOLO run. I'm looking at the data, (Dylan drowns out Nathan) I think this is it. - This is why you wanna work in post-training, because the GPU cost for training is lower. So, you can make a higher percentage of your training runs YOLO runs - Yeah. - For now. - Yeah, for now, for now. - For now, for now. (laughs) - So, some of this is fundamentally luck still. - Luck is skill in many cases. - Yeah, it looks lucky right when you're- - But the hill to climb, if you're on one of these labs and you have an evaluation and you're not crushing, there's a repeated playbook of how you improve things. There are localized improvements, which might be data improvements. And these add up into the whole model just being much better. And when you zoom in really close, it can be really obvious that this model's just really bad at this thing and we can fix it. And you just add these up. So, some of it feels like luck, but on the ground, especially with these new reasoning models we're talking to, is just so many ways that we can poke around. And normally, it's that some of them give big improvements. - The search space is near infinite. And yet, the amount of compute and time you have is very low. And you have to hit release schedules, you have to not get blown past by everyone. Otherwise, what happened with DeepSeek crushing Meta and Misral and Cohere, and all these guys, they moved too slow. They maybe were too methodical. I don't know, they didn't hit the YOLO run, whatever the reason was, maybe they weren't as skill. You can call it luck if you want, but at the end of the day, it's skill. - So, 2025 is the year of the YOLO run. It seems like all the labs are going in. - I think it's even more impressive what OpenAI did in 2022. At the time, no one believed in mixture of experts models at Google who had all the researchers. OpenAI had such little compute and they devoted all of their compute for many months, all of it, 100% for many months, to GPT-4, with a brand new architecture with no belief that, hey, let me spend a couple $100 million, which is all of the money I have, on this model. That is truly YOLO, right? - Yeah, yeah. - Now, people are like, all these training run failures that are in the media, it's like, okay, great, but actually, a huge chunk of my GPs are doing inference. I still have a bunch doing research constantly. And yes, my biggest cluster is training, but on this YOLO run, but that YOLO run is much less risky than what OpenAI did in 2022, or maybe what DeepSeek did now, or like, hey, we're just gonna throw everything at it. - The big winners throughput human history are the ones who are willing to do YOLO at some point. Okay. What do we understand about the hardware it's been trained on? DeepSeek. - DeepSeek is very interesting. A second to take us to zoom out out of who they are, first of all. High-Flyer is a hedge fund that has historically done quantitative trading in China as well as elsewhere. And they have always had a significant number of GPUs. In the past, a lot of these high frequency trading, algorithmic, quant traders, used FPGAs, but it shifted to GPUs, definitely. And there's both. But GPUs especially, and High-Flyer, which is the hedge fund that owns DeepSeek. And everyone who works for DeepSeek is part of High-Flyer to some extent. Same parent company, same owner, same CEO. They had all these resources and infrastructure for trading, and then they devoted a humongous portion of them to training models, both language models and otherwise. Because these techniques were heavily AI-influenced. More recently, people have realized, hey, trading with, even when you go back to Renaissance and all these quantitative firms, natural language processing is the key to trading really fast, understanding a press release and making the right trade. And so, DeepSeek has always been really good at this. And even as far back as 2021, they have press releases and papers saying like, hey, we're the first company in China with an A100 cluster this large. It was 10,000 A100 GPUs. This is in 2021. Now, this wasn't all for training large language models. This is mostly for training models for their quantitative aspects, or quantitative trading, as well as a lot of that was natural language processing, to be clear. And so, this is the history. So, verifiable fact is that in 2021, they built the largest cluster, at least they claim it was the largest cluster in China, 10,000 GPUs. - Before expert controls started. - Yeah. - It's like, they've had a huge cluster before any conversation of export controls. So, then you step it forward to like, what have they done over the last four years since then? Obviously, they've continued to operate the hedge fund, probably make tons of money. And the other thing is that they've leaned more and more and more into AI. the CEO, Liang Wenfeng, Liang- - You're not putting me on spot on this. We discussed this. (laughs) - Liang Feng, the CEO, he owns maybe- - We're all fam. (chuckles) - Liang Feng, he owns maybe a little bit more than half the company, allegedly, an extremely like Elon-Jensen kind of figure, where he is just involved in everything. And so, over that time period, he's gotten really in depth into AI. He actually has a bit of a like a, if you see some of his statements, a bit of an EAC vibe almost. - Total AGI vibes. And like, we need to do this, we need to make a new ecosystem of OpenAI. We need China to lead on this sort of ecosystem, because historically, the Western countries have led on software ecosystems, and straight up acknowledges, in order to do this, we need to do something different. DeepSeek is his way of doing this. Some of the translated interviews with him are fantastic. - So, he has done interviews? - Yeah. - Do you think he would do a Western interview or no? Or is there controls on the channel? - There hasn't been one yet, but I would try it. - Okay. All right. (Nathan laughing) Well, I just got a Chinese translator, so it was great. This is all push. So, fascinating figure, engineer pushing full on into AI, leveraging the success from the high frequency trading. - Very direct quotes, like we will not switch to closed source when asked about this stuff. Very long-term motivated in how the ecosystem of AI should work. And I think from a Chinese perspective, he wants a Chinese company to build this vision. - And so, this is like the, quote, unquote, \"visionary behind the company\". This hedge fund still exists, this quantitative firm. And so, DeepSeek is the sort of, solely, he got turned to this full view of like AI, everything about this. But at some point, it slowly maneuvered and he made DeepSeek. And DeepSeek has done multiple models since then. They've acquired more and more GPUs. They share infrastructure with the fund. And so, there is no exact number of public GPU resources that they have, but besides this 10,000 GPUs that they bought in 2021, and they were fantastically profitable. And then, this paper claims they did only 2,000 H800 GPUs, which are a restricted GPU that was previously allowed in China, but no longer allowed, and there's a new version. But it's basically Nvidia's H100 for China. And there's some restrictions on it, specifically around the communications sort of speed, the interconnect speed, which is why they had to do this crazy SM scheduling stuff. So, going back to that. It's like this is obviously not true in terms of their total GPU count. - Obvious available GPUs. But for this training run, you think 2,000 is the correct number or no? - So, this is where it takes a significant amount of zoning in. What do you call your training run? You count all of the research and ablations that you ran, picking all this stuff, because, yes, you can do a YOLO run, but at some level, you have to do the test at the small-scale, and then you have to do some test at medium-scale before you go to a large-scale. - Accepted practices that for any given model that is a notable advancement, you're gonna do 2 to 4x compute of the full training run in experiments alone. - So, a lot of this compute that's being scaled up is probably used in large part at this time for research. - Yeah. And research begets the new ideas that lets you get huge efficiency- - Research gets you o1. Research gets you breakthrough, so you need to bet on it. - So, some of the pricing strategy that we'll discuss has the research baked into the price. - So, the numbers that DeepSeek specifically said publicly are just the 10,000 GPUs in 2021, and then 2,000 GPUs for only the pre-training for V3. They did not discuss cost on R1. They did not discuss cost on all the other RL, for the instruct model that they made. They only discussed the pre-training for the base model, and they did not discuss anything on research and ablations. And they do not talk about any of the resources that are shared in terms of, hey, the fund is using all these GPUs. And we know that they're very profitable in that 10,000 GPUs in 2021. So, some of the research that we've found is that we actually believe they have closer to 50,000 GPUs. - We, as SemiAnalysis, so we should say - Yeah. - that you're one of the world experts in figuring out what everybody's doing in terms of the semiconductor, in terms of cluster buildouts, in terms of who's doing what in terms of training runs. So, yeah. So, that's the we. Okay, go ahead. - Yeah, sorry, sorry. We believe they actually have something closer to 50,000 GPUs. - Yeah. - [Dylan] Now, this is split across many tasks. Again, the fund, research and ablations. - For ballpark, how much would OpenAI or Anthropic had? I think the clearest example we have, because Meta is also open, they talk about order of 60k to a 100k H100 equivalent GPUs in their training clusters. - Right. So, like Llama 3, they trained on 16,000 H100s. But the "}}
%---
%[output:5a30338a]
%   data: {"dataType":"textualVariable","outputData":{"name":"ans","value":"80211"}}
%---
